# Lab 03: Linear Regression and How to minimize cost 를 TensorFlow 로 구현하기

- 190530



- 선형회귀(Linear Regression)의 비용을 최소화하는 방법을 코드로 알아본다.

#### 즉시 실행 (Eager Execution)

```
tf.enable_eager_execution()
```

TensorFlow 1.7 버전 이상에서 제공됩니다. 

TensorFlow를 대화형 명령 스타일 로 프로그래밍 할 수 있도록 해주는 것입니다.

tf.enable_eager_execution() 단 한줄이면, 그래프 기반 모드에서 즉시 실행 (Eager Execution) 모드로 변경하여 사용할 수 있습니다

없으면 에러 발생함



### Gradient descent

$$
W := W - \alpha \frac{1}{m}\sum_{i=1}^m(W(x_i) - y_i)x_i
$$

- 

$$
\frac{1}{m}\sum_{i=1}^m(W(x_i) - y_i)x_i
$$

```
gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
```



- 

$$
W := W - \alpha \frac{1}{m}\sum_{i=1}^m(W(x_i) - y_i)x_i
$$

```
descent = W - tf.multiply(alpha, gradient)
```

- 

```
import tensorflow as tf 

alpha = 0.01
gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
descent = W - tf.multiply(alpha, gradient)
W.assign(descent)
```



#### gradient_descent2.py 파일로 코드 확인



------



# Lec 04: Multi-variable Linear Regression

- 다중 선형회귀(Multi-variable Linear Regression)의 개념을 알아본다.



### Hypothesis 

- 변수가 하나 일때

$$
H(x) = Wx + b
$$

- 변수가 여러개 일때

$$
H(x_1, x_2, x_3) = w_1x_1 +w_2x_2 +w_3x_3 +b
$$



### Cost function

- 변수가 여러개 일때

$$
cost(W, b) = \frac{1}{m}\sum_{i=1}^m(H(x_1, x_2, x_3) - y_i)^2
$$

### Multi-variable

- 변수가 늘어날수록 가중치의 수도 늘어난다.

$$
H(x_1, x_2, x_3) = w_1x_1 +w_2x_2 +w_3x_3 +b
$$


$$
H(x_1, x_2, x_3, ..., x_n) = w_1x_1 +w_2x_2 +w_3x_3 + ... + w_nx_n+b
$$


### Matrix(행렬) 

- 변수와 가중치의 수가 너무 늘어나서 사용이 불편해 질때 Matrix 를 사용하면 편하다.



### Matrix multiplication

- Dot Product (매트릭스의 곱셈)

$$
\left( x_1 \quad x_2 \quad x_3 \right) \ \cdot \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix} = (x_1w_1 + x_2w_2 + x_3w_3)
$$

- 

$$
H(X) = XW
$$

### Hypothesis using matrix



### Many X instances

데이터의 양이 많아질수록 더 간편해 보이는 장점이 있다.



- 앞 매트릭스의 column와 뒤 매트릭스의 row 가 같아야 곱이 가능하다.



### WX  vs  XW

- Lecture(theory)

$$
H(x) = Wx + b
$$

$$
h_0(x) = 0_1x + 0_0
$$

$$
f(x) = ax + b
$$



- Implementation(Tensorflow)

$$
H(X) = XW
$$

매트릭스 계산이므로 순서가 다름



----



# Lab 04: Multi-variable Linear Regression 를 TensorFlow 로 구현하기

- 다중 선형회귀(Multi-variable Linear Regression)를 코드로 구현해본다.



### with Matrix

- matrix를 사용할때

$$
H(X) = XW
$$

```
# initialize w
w1 = tf.Variable(tf.random_normal([1]))
w2 = tf.Variable(tf.random_normal([1]))
w3 = tf.Variable(tf.random_normal([1]))
```

```
# hypothesis, prediction function
hypothesis = w1*x1 + w2*x2 + w3*x3 + b 
```

```
# update w1, w2, w3
w1.assign_sub(learning_rate * w1_grad)
w2.assign_sub(learning_rate * w2_grad)
w3.assign_sub(learning_rate * w3_grad)
```



- matrix를 사용 안할때

$$
\left( x_1 \quad x_2 \quad x_3 \right) \ \cdot \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix} = (x_1w_1 + x_2w_2 + x_3w_3)
$$

```
# initialize w
w = tf.Variable(tf.random_normal([3, 1]))
```

```
# hypothesis, prediction function
tf.matmul(X, w) + b
```

```
# update parameters (w and b)
w.assign_sub(learning_rate * W_grad)
```



### multi_varialble_linear_regression.py 파일로 코드 확인

matrix를 사용 안할때



### multi_varialble_linear_regression2.py 파일로 코드 확인

matrix를 사용할때