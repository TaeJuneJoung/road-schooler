# Lec 05-1: Logistic Regression/Classification 의 소개

- 190531



- 로지스틱 회귀/분류(Logistic Regression/Classification)의 개념을 알아본다.



### Binary(Multi-class) Classification

variable is either 0 or 1

- exam: pass or fail
- spam: not spam or spam
- face: real of fake
- tumor: not malignant or malignant



### logistic 와 linear의 차이점

- logistic

Discrete(counted)

Shoe Size / The number of workers in a company

데이터들간에 구분이 가능함

- linear

Continuous(Measured)

Time/ Weight/ Height

데이터들이 연속적임



### Sigmoid(Logistic) function



### Hypothesis Representation

x -> Linear function -> Logisgic function -> Decision Boundary -> Y





# Lec 05-2: Logistic Regression/Classification 의 cost 함수, 최소화

- 로지스틱 회귀/분류(Logistic Regression/Classification)의 비용함수를 최소화 하는 방법을 알아본다.



### Cost function

- 가설과 실제의 차이가 최대한 적도록 만들어야함



- A convex logistic regression cost function

$$
Cost(h_0(x), y) = -ylog(h_0(x)) - (1-y)log(1-h_0(x)))
$$

- Tensorflow code

```
cost = -tf.reduce_mean(labels * tf.log(hypothesis) + (1 - labels) * tf.log(1-hypothesis))
```



### Optimizer

- How to minimize the cosst function



- 그래프의 경사값을 통해서 cost가 0에 가까운곳을 찾을 수 있음



- 기울기가 최적이 되도록 조금씩 repeat



- Tensorflow code

```
def grad(hypothesis, labels):
	with tf.GradientTape() as tape:
		loss_value = loss_fn(hypothesis, labels)
	return tape.gradient(loss_value, [W, b])
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))
```



# Lab 05-3: Logistic Regression/Classification 를 TensorFlow 로 구현하기

- 로지스틱 회귀/분류(Logistic Regression/Classification)를 코드로 구현한다.



### [Logistic Regression ](https://github.com/deeplearningzerotoall/TensorFlow/blob/master/lab-05-1-logistic_regression-eager.ipynb)



#### Logistic.py 파일로 코드 확인



