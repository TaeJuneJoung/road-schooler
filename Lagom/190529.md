# School of AI : 라이브 코딩으로 딥러닝 최신 트렌드 배우기

- 190529



## 딥러닝 시작하기 - 예측 하는 방법

**머신러닝의 종류** 

- 지도 학습(Supervised learning)
  - 레이블된 데이터셋을 사용하여 학습 
  - 레이블과 데이터간의 관계를 학습 
  - 자동차 번호판 인식, 동물의 종류 인식 등등 
- 비지도 학습(Unsupervised learning)
  - 레이블이 안된 데이터셋을 사용하여 학습
- 강화 학습 (Reinforcement learning)
  - 시행 착오를 통해 환경과 상호작용하며 피드백을 통해 학습



- 선형 회귀 (Linear Regression)





## 모두를 위한 머신러닝/딥러닝 강의



### TensorFlow의 설치 및 기본적인 operations



- cmd 에서 설치

````
pip install --upgrade pip

pip install tensorflow
````



- 버전 확인

```
>>> import tensorflow as tf
>>> tf.__version__
```



### Simple Linear Regression

머신러닝에서 핵심임



#### 선형 회귀(Linear Regression)

- Regression toward the mean

전체 평균으로 되돌아간다



- 데이터를 가장 잘 대변하는 직선의 방정식을 찾는것

y = ax + b

a : 기울기

b : 직선의 y절편



#### 가설(Hypothesis)

$$
H(x) = Wx + b
$$



#### 비용함수(Cost function)

$$
cost(W) = \frac{1}{m} \sum_{i=1}^m (Wx_i - y_i)^2
$$

$$
H(x) = Wx + b
$$


$$
cost(W, b) = \frac{1}{m} \sum_{i=1}^m (H(x_i) - y_i)^2
$$




- Goal

Minimize cost





### Lab 02: Simple Linear Regression 를 TensorFlow 로 구현하기



- tf.reduce_mean()

```
v = [1. , 2. , 3. , 4. ]

tf.reduce_mean(v)
```



- tf.square()

```
tf.square(3)
```



#### Gradient descent

경사 하강법

w 와 b를 찾는 알고리즘



- A.assign_sub(B)

```
A = A - B
A -= B
```



- 

```
# learning_rate initialize
learning_rate = 0.01

# Gradient descent
with tf.GradientTape() as tape:
    hypothesis = W * x_data + b
    cost = tf.reduce_mean(tf.square(hypothesis - y_data))

W_grad, b_grad = tape.gradient(cost, [W, b])

W.assign_sub(learning_rate * W_grad)
b.assign_sub(learning_rate * b_grad)
```

조금씩 조금씩 가정했던 값을 수정해서 정확도가 높은 값으로 변경시킴(한발자국)



- 

```
import tensorflow as tf
tf.enable_eager_execution()
x_data = [1, 2, 3, 4, 5]
y_data = [1, 2, 3, 4, 5]

W = tf.Variable(2.9)
b = tf.Variable(0.5)

# learning_rate initialize
learning_rate = 0.01

for i in range(100):
    # Gradient descent
    with tf.GradientTape() as tape:
        hypothesis = W * x_data + b
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))
    W_grad, b_grad = tape.gradient(cost, [W, b])
    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)
    if i % 10 == 0:
        print("{:5} | {:10.4f} | {:10.4} |{:10.6f}".format(i, W.numpy(), b.numpy(), cost))

```



```
0 |     2.4520 |      0.376 | 45.660004
10 |     1.1036 |   0.003398 |  0.206336
20 |     1.0128 |   -0.02091 |  0.001026
30 |     1.0065 |   -0.02184 |  0.000093
40 |     1.0059 |   -0.02123 |  0.000083
50 |     1.0057 |   -0.02053 |  0.000077
60 |     1.0055 |   -0.01984 |  0.000072
70 |     1.0053 |   -0.01918 |  0.000067
80 |     1.0051 |   -0.01854 |  0.000063
90 |     1.0050 |   -0.01793 |  0.000059
```

for 문이 계속 될수록 cost(에러)값이 적어짐



```
print(W * 5 + b)
print(W * 2.5 + b)
```

```
tf.Tensor(5.0066934, shape=(), dtype=float32)
tf.Tensor(2.4946523, shape=(), dtype=float32)
```



### Lec 03: Linear Regression and How to minimize cost

선형 회귀의 비용을 최소화 하는 방법을 알아본다

#### How to minimize cost



- Hypothesis

가설, 예측

Simplified hypothesis
$$
H(x) = Wx
$$
이전 수식에서 b를 생략

- Cost

가설과 실제 데이터의 차이(error)의 제곱의 평균

cost 가 적을수록 실제 데이터를 잘 반영하는 것

Simplified hypothesis 
$$
cost(W) = \frac{1}{m}\sum_{i=1}^m(Wx_i - y_i)^2
$$
이전 수식에서 b를 생략



- cost 함수를 그래프로 그리면 밑으로 볼록한 그래프가 그려짐
- cost가 최저점일 때를 찾아야함



- How it works?

Start with initial guesses

​	start at 0, 0(or any other value)

​	Keeping changing W and b a little bit to try and reduce cost(W, b)

Each time you change the parameters, you select the gradient which reduces cost(W, b) the most possible

Repeat

Do so until you converge to a local minimum

Has an interesting property

​	Where you start can determine which minimum you end up



- 동작방법

최초의 추정을 통해 w와 b의 값을 정한 후 cost가 줄어드는 방향으로 w와 b값을 지속적으로 업데이트를 함

cost가 최소가 되는 방향으로 업데이트

 최소점에 도달했다고 판단될때 까지 업데이트를 반복



#### Formal definition

$$
cost(W, b) = \frac{1}{m} \sum_{i=1}^m (H(x_i) - y_i)^2
$$

​	=>
$$
cost(W, b) = \frac{1}{2m} \sum_{i=1}^m (H(x_i) - y_i)^2
$$
2m으로 바꿔준 이유:

m이 2m이나 3m 이어도 cost의 특성에는 영향을 주지 않음

나중에 기울기를 구하기 위해서 미분을 할껀데 그때 에러의 지수을 없애고 좀 더 간략화 시키기 위해서



- W에 대해 편미분을 하면

$$
W := W - \alpha\frac{\partial}{\partial W} \frac{1}{2m} \sum_{i=1}^m (W(x_i)-y_i)^2
$$


$$
W := W - \alpha\frac{1}{2m} \sum_{i=1}^m 2(W(x_i)-y_i) x_i
$$
Gradient descent algorithm
$$
W := W - \alpha\frac{1}{m} \sum_{i=1}^m (W(x_i)-y_i) x_i
$$


learning_rate(학습률) : α

값이 클수록 크게 움직임, 적적함 값을 선택하는게 중요함



- Gradient descent algorithm

$$
W := W - \alpha\frac{\partial}{\partial W} cost(W)
$$



- Convex function

local minimum이 여러개인 경우

local minimum 와 local minimum이 다른 경우가 발생함

Gradient descent algorithm을 사용할 수 없음



- Convex function

global minimum와  local minimum이 일치하는 경우

Gradient descent algorithm을 사용할 수 있음



